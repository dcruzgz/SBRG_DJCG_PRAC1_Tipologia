{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB Scraping - OPERADORES ECOLÓGICOS ANDALUCÍA  \n",
    "\n",
    "@SARA BELÉN RAMOS GONZÁLEZ y DANIEL JESÚS CRUZ GARZÓN\n",
    "\n",
    "\n",
    "### <i> Requisitos para que funcione el script </i>\n",
    "\n",
    "+ Selenium (con el driver gecko) \n",
    "\n",
    "    `pip install selenium`\n",
    "    \n",
    "    https://github.com/mozilla/geckodriver/releases\n",
    "    \n",
    "    El ejectuable geckodriver.exe debe estar alojado en el directorio de trabajo del notebook o agregarlo a la variable PATH\n",
    "    \n",
    "    \n",
    "+ Katalon Recorder (plugin para firefox que emula la navegacion humana y graba en python esta navegación)\n",
    "\n",
    "    https://www.katalon.com/resources-center/blog/katalon-automation-recorder/\n",
    "\n",
    "+ Habrá que definir el path correspondiente en cada caso\n",
    "\n",
    "### Descripción\n",
    "\n",
    "Se recorre por cada elemento del listado de provincias de la Comunidad Autónma de Andalucía todas las páginas con la información que queremos extraer. Para ello, implementaremos un bucle for que recorre provincias con un bucle while para recorrer paginas hasta que se acaben. \n",
    "La URL de la pagina elegida para hacer scraping es: https://servicio.mapama.gob.es/regoe/Publica/Operadores.aspx\n",
    "\n",
    "### Inconvenientes\n",
    "\n",
    "#### * Primer inconveniente\n",
    "\n",
    "El enlace con \"...\" para avanzar a los siguientes enlaces tiene la forma de:\n",
    "\n",
    "```python \n",
    "driver.find_element_by_link_text('...').click() # Hacemos click en la pagina de continuacion\n",
    "```\n",
    "sin embargo cuando se tienen dos enlaces con \"...\" (uno para avanzar y otro para volver atrás), produce que si utilizamos la misma forma que la anterior vuelva a los enlaces ya pasados y este bucle sea infinito. Por lo tanto, el enlace con \"...\" de avance cambia y es el siguiente:\n",
    "\n",
    "```python\n",
    "driver.find_element_by_xpath(\"(.//*[normalize-space(text()) and normalize-space(.)='...'])[1]/following::a[13]\").click()\n",
    "```\n",
    "\n",
    "#### * Segundo inconveniente\n",
    "\n",
    "Para la provincia de Almería surge error al no encontrar la tabla de información debido a que tarda en actualizarse la página después de haber producido una acción de click (siguiente provincia o página). Hay dos formas de solucionarlo:\n",
    "\n",
    "- Si la pagina siguiente tiene un tag con id distinto al de la página de la cual proviene podemos esperar que cargue hasta que encuentre este id en la página que está cargando:\n",
    "\n",
    "```python\n",
    "# Tenemos el id de la siguiente página de carga\n",
    "WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.ID, \"CphCuerpoPagina_GVCertificados_btnCertificadoES_0\"))\n",
    "```\n",
    "\n",
    "- Si la página es prácticamente igual que la anterior y sólo cambia el contenido de la tabla, podemos hacer un sleep forzado a un tiempo constante.\n",
    "\n",
    "```python\n",
    "time.sleep(20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos los módulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los módulos necesarios\n",
    "import os\n",
    "from _csv import reader\n",
    "\n",
    "# Aprovechamos y nos situamos en el directorio donde extraeremos nuestros datos\n",
    "os.chdir(\"C:\\\\Users\\\\sara.ramos\\\\Anaconda3\\\\VS_NOTEBOOKS\\\\\")  # colocamos nuestro path correspondiente en cada caso\n",
    "\n",
    "# Importamos el resto de módulos necesarios\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import unittest, time, re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos la función de espera de página\n",
    "def espera_nueva_pagina():\n",
    "    global driver\n",
    "    try:  # Comprobamos que carga bien la pagina, condicionando que aparezca una seccion de la tabla con id\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.ID, \"CphCuerpoPagina_GVCertificados_btnCertificadoES_0\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"Fallo al cargar la página\")\n",
    "\n",
    "# Definimos la función de hacer click en la página siguiente\n",
    "def click_sig_pag():\n",
    "    global driver\n",
    "    global ultima_pagina\n",
    "    global siguiente_pagina\n",
    "    global logging\n",
    "\n",
    "    try:  # Condicionamos a que la página cargue\n",
    "        driver.find_element(By.LINK_TEXT, str(siguiente_pagina)).click()\n",
    "    except:\n",
    "        print(\"Esta página no existe: \" + str(siguiente_pagina) + \" probando siguiente pagina + 1\")\n",
    "        # Almacenamos el paso llevado a cabo en un archivo de texto a modo de log\n",
    "        logging.info(\"Esta página no existe: \" + str(siguiente_pagina) + \" probando siguiente pagina + 1\")\n",
    "        if siguiente_pagina > int(ultima_pagina):\n",
    "            driver.find_element(By.LINK_TEXT, str(siguiente_pagina + 1)).click()\n",
    "            siguiente_pagina += 1\n",
    "\n",
    "# Definimos la función para exportar los datos en la tabla correspondiente\n",
    "def exportInfoTable(i, soup, outputFile):\n",
    "    n_filas = 0\n",
    "    cuenta = 0\n",
    "\n",
    "    for sub_tag_i in soup.find_all('tr'):\n",
    "        cuenta += 1\n",
    "        # print (\"PRUEBA DE SUB_TAG_I\",sub_tag_i,\"tag:\", cuenta) #PRUEBAS OUTPUT\n",
    "        # print (\"LEN SUB_TAG_I LONGITUD TOTAL\",len(sub_tag_i),\"tag:\", cuenta) #PRUEBAS OUTPUT\n",
    "\n",
    "        global logging\n",
    "\n",
    "        row = \"\"\n",
    "        row_1 = \"\"\n",
    "        row_2 = \"\"\n",
    "\n",
    "        if len(sub_tag_i) >= 1:\n",
    "            if len(row_1) > 0:  # Solo volcamos si la fila no esta vacia\n",
    "                n_filas += 1\n",
    "                outputFile.write(row_1)\n",
    "                outputFile.flush()\n",
    "\n",
    "            for subtag_x in sub_tag_i.find_all('th'):\n",
    "                row_x = re.search('>(.+?)</th>', str(subtag_x)).group(1)\n",
    "                row_2 += '\"' + row_x + '\";'\n",
    "\n",
    "            if i == 0:  # re.sub(r\"\\s+\", \"\", str(row_2)) != re.sub(r\"\\s+\", \"\", nombres) and len(re.sub(r\"\\s+\", \"\", str(row_2))) > 0:\n",
    "                outputFile.write(row_2.replace('\" \";', ''))\n",
    "                outputFile.flush()\n",
    "\n",
    "            for sub_tag_j in sub_tag_i.find_all('td'):\n",
    "                sub_tag_j = sub_tag_j.text.replace(\";\", \"\").replace('\"', '').replace(\"\\n\",\n",
    "                                                                                     \" \").strip()  # si contiene un caracter separador de columna o comilla lo quito\n",
    "                row += '\"' + sub_tag_j + '\";'\n",
    "\n",
    "            row = row[0:len(row) - 1]  # Quitamos el último separador ';\"' innecesario\n",
    "            row += \"\\n\"\n",
    "\n",
    "            if (len(row) > 0 and i == 0) or (len(row) > 1 and i != 0):  # Solo volcamos si la fila no esta vacia\n",
    "                n_filas += 1\n",
    "                if not re.match(r'^[\"]\\d+.*|^[\"][.]{3}.*|^[\"]Visualizar.*',\n",
    "                                row):  # No incorporamos las filas con los numeros de página\n",
    "                    outputFile.write(row.replace('\"\";', ''))\n",
    "                    outputFile.flush()  # Limpiamos el buffer interno\n",
    "\n",
    "    print(\"Numero de filas extraidas: \", str(n_filas))  # OUTPUT FILAS\n",
    "\n",
    "\n",
    "# Definimos la variable con el conjunjto de los hombres de las provincias disponibles            \n",
    "prov_name = ['Almería', 'Cádiz', 'Córdoba', 'Granada', 'Huelva', 'Jaén', 'Málaga', 'Sevilla']\n",
    "\n",
    "# Creamos el archivo donde vamos insertando los nuevos registros\n",
    "outputFile = open(\"./outputFile_Andalucia_completo.csv\", \"a\")\n",
    "es_primer_grupo_paginas = True\n",
    "siguiente_pagina = 1\n",
    "soup_text_ant = \"\"\n",
    "\n",
    "# También crearemos un archivo para almacenar el log de nuestra ejecución\n",
    "logging.basicConfig(filename='log_Andalucia_completo.log', level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------#\n",
    "#      >>>>> MAIN <<<<<    #\n",
    "# -------------------------#\n",
    "\n",
    "# Creamos una instancia al navegador firefox\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(60)\n",
    "\n",
    "# Entramos en la pagina\n",
    "driver.get(\"https://servicio.mapama.gob.es/regoe/Publica/Operadores.aspx\")\n",
    "\n",
    "# Nos situamos en el desplegable de CCAA\n",
    "driver.find_element(By.ID,\"CphCuerpoPagina_ddlCCAA\").click()\n",
    "\n",
    "# Seleccionamos la CCAA \"Andalucía\"\n",
    "Select(driver.find_element(By.ID,\"CphCuerpoPagina_ddlCCAA\")).select_by_visible_text('Andalucía')\n",
    "\n",
    "# Recorremos todas las provincias del listado definiendo el rango total\n",
    "for i in range(1, (len(prov_name)+1)):\n",
    "    es_primer_grupo_paginas = False\n",
    "    siguiente_pagina = 1\n",
    "\n",
    "    option = i + 1\n",
    "    provincia = prov_name[i - 1]\n",
    "    print(\"Provincia: \" + provincia + \" option: \" + str(option-1) + \"\\n\\n\")\n",
    "    \n",
    "    # if  provincia == 'Almería' : # PRUEBAS SIN ALMERÍA PARA AGILIZAR\n",
    "    #     continue    # continue here\n",
    "\n",
    "    # Seleccionamos una provincia\n",
    "    Select(driver.find_element(By.ID,\"CphCuerpoPagina_ddlProvincias\")).select_by_visible_text(provincia)\n",
    "\n",
    "    # Hacemos click en la seleccion de la provincia\n",
    "    driver.find_element(By.XPATH,\n",
    "        \"(.//*[normalize-space(text()) and normalize-space(.)='Provincia'])[1]/following::option[\" + str(\n",
    "            option) + \"]\").click()\n",
    "\n",
    "    # Hacemos click en el boton de buscar\n",
    "    driver.find_element(By.ID, \"CphCuerpoPagina_btnBuscar\").click()\n",
    "    # Almacenamos el paso llevado a cabo en un archivo de texto a modo de log\n",
    "    logging.info('Click en buscar ' + provincia)\n",
    "    espera_nueva_pagina()\n",
    "\n",
    "    index_pg = 0  # Índice del nÚmero de páginas cargada\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Condicionamos la provincia que da error por espera de página\n",
    "            if (provincia == 'Almería'):\n",
    "                print(\"Esperando actualizacion de la pagina por 20 segundos...\")\n",
    "                time.sleep(20)\n",
    "\n",
    "            # Extraemos el html del resultado de la acción anterior\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            # Extraemos el contenido de la tabla de la pagina y exportamos a CSV\n",
    "            exportInfoTable(index_pg, soup, outputFile)\n",
    "            # Almacenamos el paso llevado a cabo en un archivo de texto a modo de log\n",
    "            logging.info('Extraccion tabla de ' + provincia + ' pagina ' + str(siguiente_pagina))\n",
    "\n",
    "            # Extraemos la información de los links a las siguientes páginas\n",
    "            soup_pag = soup.findAll('tr', {\"class\": \"GridViewPaginacion\"})  # Nos situamos en la tabla de paginaciones\n",
    "            soup_pag = soup_pag[0].findAll('a')  # Extraemos todos los enlaces a cada número de página\n",
    "            ultima_pagina = soup_pag[len(soup_pag) - 2].text\n",
    "            continuacion_pagina = soup_pag[len(soup_pag) - 1].text\n",
    "            print(\"Siguiente pág: \" + str(siguiente_pagina) + \" // Última pag: \" + str(\n",
    "                ultima_pagina) + \" // Continuación pág: \" + continuacion_pagina)\n",
    "            print(50 * \"-\")\n",
    "\n",
    "            # Si no hay enlace de continuación entonces la ultima página es numérica\n",
    "            if continuacion_pagina != '...':\n",
    "                ultima_pagina = continuacion_pagina\n",
    "\n",
    "            siguiente_pagina += 1\n",
    "\n",
    "            # Si la siguiente pagina sobrepasa la última página numerada entonces definimos su acción\n",
    "            if siguiente_pagina > int(ultima_pagina):\n",
    "\n",
    "                # si la última página es la de continuacion \"\"...\"\"\n",
    "                if continuacion_pagina == '...':\n",
    "                    if es_primer_grupo_paginas == True:\n",
    "                        driver.find_element(By.LINK_TEXT,'...').click()  # Hacemos click en la página de continuación\n",
    "                        es_primer_grupo_paginas = False\n",
    "                    else:  # cuando hay dos enlaces con \"...\" el de la derecha es el del siguiente xpath:\n",
    "                        try:\n",
    "                            driver.find_element(By.XPATH,\n",
    "                                \"(.//*[normalize-space(text()) and normalize-space(.)='...'])[1]/following::a[13]\").click()\n",
    "                        except Exception as inst:\n",
    "                            driver.find_element(By.LINK_TEXT,\n",
    "                                '...').click()  # Hacemos click en la página de continuación\n",
    "\n",
    "                    # siguiente_pagina += 1 # sumamos uno mas paa saltar 2 puesto que la \"...\" si cuenta\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "\n",
    "                # Hacemos click en la siguiente página numérica\n",
    "                # click_sig_pag()\n",
    "                print(\"hago click en: \" + str(siguiente_pagina))\n",
    "                driver.find_element(By.LINK_TEXT, str(siguiente_pagina)).click()\n",
    "            index_pg += 1\n",
    "\n",
    "        except:\n",
    "            print('Error en ' + provincia + ' pagina ' + str(siguiente_pagina))\n",
    "            # Almacenamos el paso llevado a cabo en un archivo de texto a modo de log\n",
    "            logging.info('Error en ' + provincia + ' pagina ' + str(siguiente_pagina))\n",
    "\n",
    "outputFile.close()\n",
    "driver.quit()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6451c4698a17ceec98555ed87a7b25183328e30c768191598191df2992c7b94"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
